# Compte rendu réunion du 28/12/2018

Contrôle de l'espace latent en temps réel
-----------------------------------------

Un projet sympa et pas cher (quoique) pour exploiter ce type de modèle serait de créer un **sampler latent** sous la forme d'un external `puredata`.

De ce fait on aurait plusieurs pôles de travail à définir:

* **Machine learning**
  * Finir benchmark des divergences
  * Modifier l'architecture du modèle pour trouver des trucs qui marcherait mieux
  * Entrainer les modèles sur différents dataset, de manière semi-supervisée ou non
  * Projeter l'espace latent sur un espace d'hyperparamètres (sons profonds, clairs, durs, sales...)
  * Bosser sur la reconstruction avec le MCNN
  * Créer un modèle complet **V(W)AE --> MCNN**
* **Dataset**
  * Prendre en main le dataset **SOL**
  * Prendre des décisions sur la durée, les annotations, etc...
  * Bosser sur les types de préprocessing à appliquer au dataset
  * CSOUND? PUREDATA?
* **Realtime**
  * Implémentation d'un wrapper permettant l'exploitation d'un modèle `pytorch` au sein d'un external puredata.
  * Gestion de multi-buffers, polyphonie, ...
  * Optimisation, temps réel, contrôle via MIDI, ...

La répartition des tâches est la suivante:

**Martin**: Realtime

**Thibault**: Dataset

**JB / Antoine**: Machine learning

Evidemment, on se tient au courant de l'avancée de chaque partie, et on peut bosser sur la partie des autres égalements.

Export d'un modèle python à C++
-------------------------------

La déclaration du modèle se fait un peu différemment: le module hérite non plus de `nn.Module` mais de `torch.jit.ScriptModule`. Pour l'exporter il suffit de faire un truc du genre:

```python
class Model(torch.jit.ScriptModule):
  def __init__(self):
    super(Model,self).__init__()
    # On définit l'architecture du modèle
    self.layer1 = nn.Linear(128,64)

  @torch.jit.script_method
  def forward(self, inp):
    # Déclaration de la méthode forward, décorée par jit.script_method
    return self.layer1(inp)

model = Model()
model.save("model.pt")
```

De cette manière on peut utiliser le modèle en C++:

```c++
#include <torch/script.h> // One-stop header.

#include <iostream>
#include <memory>

int main(int argc, const char* argv[]) {
  if (argc != 2) {
    std::cerr << "usage: example-app <path-to-exported-script-module>\n";
    return -1;
  }

  // Deserialize the ScriptModule from a file using torch::jit::load().
  std::shared_ptr<torch::jit::script::Module> module = torch::jit::load(argv[1]);

  assert(module != nullptr);
  std::cout << "ok\n";
}
```

Toute la doc expliquant le passage de l'un à l'autre se trouve [par ici](https://pytorch.org/tutorials/advanced/cpp_export.html).


Prochaines réunion
------------------

Début de l'année prochaine, en même temps que la réu d'automatique.
